{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "53acfe4c-7237-409d-aa91-398f147da0de",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     /Users/ismaildibirov/nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import re\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "nltk.download('vader_lexicon')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dfc2b803-89e3-4273-987f-90cdb0905c25",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>_id</th>\n",
       "      <th>channel</th>\n",
       "      <th>text</th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6579a3c0316b8a2a5b0c0827</td>\n",
       "      <td>cryptowhalesreal</td>\n",
       "      <td>I hope eveyone could get this fabulous profit ...</td>\n",
       "      <td>2023-12-12T09:40:12.000Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6579a3c0316b8a2a5b0c0828</td>\n",
       "      <td>cryptowhalesreal</td>\n",
       "      <td>üéÅ #FREE_SIGNAL \\n\\nüü£ Future Signal üü£\\nüìà BUY\\n‚ùá...</td>\n",
       "      <td>2023-12-11T12:39:33.000Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6579a3c0316b8a2a5b0c0829</td>\n",
       "      <td>cryptowhalesreal</td>\n",
       "      <td>Another 40% profit after the entry for a buy s...</td>\n",
       "      <td>2023-12-10T13:24:29.000Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6579a3c0316b8a2a5b0c082a</td>\n",
       "      <td>cryptowhalesreal</td>\n",
       "      <td>üü£ Future Signal üü£\\nüìà BUY\\n‚ùáÔ∏è #SOL - USDT\\nüè¢ Ex...</td>\n",
       "      <td>2023-12-10T13:24:20.000Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6579a3c0316b8a2a5b0c082b</td>\n",
       "      <td>cryptowhalesreal</td>\n",
       "      <td>This is the correct way of starting the weeken...</td>\n",
       "      <td>2023-12-09T07:54:00.000Z</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        _id           channel  \\\n",
       "0  6579a3c0316b8a2a5b0c0827  cryptowhalesreal   \n",
       "1  6579a3c0316b8a2a5b0c0828  cryptowhalesreal   \n",
       "2  6579a3c0316b8a2a5b0c0829  cryptowhalesreal   \n",
       "3  6579a3c0316b8a2a5b0c082a  cryptowhalesreal   \n",
       "4  6579a3c0316b8a2a5b0c082b  cryptowhalesreal   \n",
       "\n",
       "                                                text                      date  \n",
       "0  I hope eveyone could get this fabulous profit ...  2023-12-12T09:40:12.000Z  \n",
       "1  üéÅ #FREE_SIGNAL \\n\\nüü£ Future Signal üü£\\nüìà BUY\\n‚ùá...  2023-12-11T12:39:33.000Z  \n",
       "2  Another 40% profit after the entry for a buy s...  2023-12-10T13:24:29.000Z  \n",
       "3  üü£ Future Signal üü£\\nüìà BUY\\n‚ùáÔ∏è #SOL - USDT\\nüè¢ Ex...  2023-12-10T13:24:20.000Z  \n",
       "4  This is the correct way of starting the weeken...  2023-12-09T07:54:00.000Z  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"../data/capstone.messages.csv\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bdd8cbfd-08d9-484d-9cfd-7e7aba0eb45e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>cleaned_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I hope eveyone could get this fabulous profit ...</td>\n",
       "      <td>i hope eveyone could get this fabulous profit ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>üéÅ #FREE_SIGNAL \\n\\nüü£ Future Signal üü£\\nüìà BUY\\n‚ùá...</td>\n",
       "      <td>free signal future signal buy tia usdt exchan...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Another 40% profit after the entry for a buy s...</td>\n",
       "      <td>another 40 profit after the entry for a buy si...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>üü£ Future Signal üü£\\nüìà BUY\\n‚ùáÔ∏è #SOL - USDT\\nüè¢ Ex...</td>\n",
       "      <td>future signal buy sol usdt exchange all avail...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>This is the correct way of starting the weeken...</td>\n",
       "      <td>this is the correct way of starting the weeken...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>üü£ Future Signal üü£\\nüìà BUY\\n‚ùáÔ∏è #SXP - USDT\\nüè¢ Ex...</td>\n",
       "      <td>future signal buy sxp usdt exchange all avail...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Semms #YGG doen't have enough strength. We jus...</td>\n",
       "      <td>semms ygg doen t have enough strength we just ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>The price had a bearish shadow; don't worry üòÆ‚Äç...</td>\n",
       "      <td>the price had a bearish shadow don t worry nti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>üéÅ #FREE_SIGNAL \\n\\nüü£ Future Signal üü£\\nüìà BUY\\n‚ùá...</td>\n",
       "      <td>free signal future signal buy ygg usdt exchan...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>We keep the Monday Blues away with profitable ...</td>\n",
       "      <td>we keep the monday blues away with profitable ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  \\\n",
       "0  I hope eveyone could get this fabulous profit ...   \n",
       "1  üéÅ #FREE_SIGNAL \\n\\nüü£ Future Signal üü£\\nüìà BUY\\n‚ùá...   \n",
       "2  Another 40% profit after the entry for a buy s...   \n",
       "3  üü£ Future Signal üü£\\nüìà BUY\\n‚ùáÔ∏è #SOL - USDT\\nüè¢ Ex...   \n",
       "4  This is the correct way of starting the weeken...   \n",
       "5  üü£ Future Signal üü£\\nüìà BUY\\n‚ùáÔ∏è #SXP - USDT\\nüè¢ Ex...   \n",
       "6  Semms #YGG doen't have enough strength. We jus...   \n",
       "7  The price had a bearish shadow; don't worry üòÆ‚Äç...   \n",
       "8  üéÅ #FREE_SIGNAL \\n\\nüü£ Future Signal üü£\\nüìà BUY\\n‚ùá...   \n",
       "9  We keep the Monday Blues away with profitable ...   \n",
       "\n",
       "                                        cleaned_text  \n",
       "0  i hope eveyone could get this fabulous profit ...  \n",
       "1   free signal future signal buy tia usdt exchan...  \n",
       "2  another 40 profit after the entry for a buy si...  \n",
       "3   future signal buy sol usdt exchange all avail...  \n",
       "4  this is the correct way of starting the weeken...  \n",
       "5   future signal buy sxp usdt exchange all avail...  \n",
       "6  semms ygg doen t have enough strength we just ...  \n",
       "7  the price had a bearish shadow don t worry nti...  \n",
       "8   free signal future signal buy ygg usdt exchan...  \n",
       "9  we keep the monday blues away with profitable ...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Correcting the function to effectively remove all hanging 'n's\n",
    "def clean_text_final_corrected(text):\n",
    "    # Removing line breaks and replacing with space\n",
    "    text = text.replace('\\n', ' ')\n",
    "    # Removing any standalone 'n' characters that might have been left from '\\n'\n",
    "    text = re.sub(r'\\b[n]\\b', ' ', text)\n",
    "    # Remaining cleaning steps\n",
    "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
    "    text = re.sub(r'\\@\\w+|\\#', '', text)\n",
    "    text = re.sub(r'[^a-zA-Z0-9]', ' ', text)\n",
    "    text = re.sub(r'\\s+', ' ', text)  # Removing extra space\n",
    "    return text.lower()\n",
    "\n",
    "# Reapplying the corrected text cleaning function\n",
    "data['cleaned_text'] = data['text'].apply(clean_text_final_corrected)\n",
    "\n",
    "# Displaying the first few rows of the updated cleaned data\n",
    "data[['text', 'cleaned_text']].head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "27e4ff88-9305-41f3-9651-20d6271683d8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cleaned_text</th>\n",
       "      <th>manual_tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>i hope eveyone could get this fabulous profit ...</td>\n",
       "      <td>[i, hope, eveyone, could, get, this, fabulous,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>free signal future signal buy tia usdt exchan...</td>\n",
       "      <td>[free, signal, future, signal, buy, tia, usdt,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>another 40 profit after the entry for a buy si...</td>\n",
       "      <td>[another, 40, profit, after, the, entry, for, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>future signal buy sol usdt exchange all avail...</td>\n",
       "      <td>[future, signal, buy, sol, usdt, exchange, all...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>this is the correct way of starting the weeken...</td>\n",
       "      <td>[this, is, the, correct, way, of, starting, th...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        cleaned_text  \\\n",
       "0  i hope eveyone could get this fabulous profit ...   \n",
       "1   free signal future signal buy tia usdt exchan...   \n",
       "2  another 40 profit after the entry for a buy si...   \n",
       "3   future signal buy sol usdt exchange all avail...   \n",
       "4  this is the correct way of starting the weeken...   \n",
       "\n",
       "                                       manual_tokens  \n",
       "0  [i, hope, eveyone, could, get, this, fabulous,...  \n",
       "1  [free, signal, future, signal, buy, tia, usdt,...  \n",
       "2  [another, 40, profit, after, the, entry, for, ...  \n",
       "3  [future, signal, buy, sol, usdt, exchange, all...  \n",
       "4  [this, is, the, correct, way, of, starting, th...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Manual tokenization function\n",
    "def manual_tokenize(text):\n",
    "    # Simple tokenization by splitting on spaces (this is less sophisticated than NLTK's word_tokenize)\n",
    "    return text.split()\n",
    "\n",
    "# Applying manual tokenization to the cleaned text\n",
    "data['manual_tokens'] = data['cleaned_text'].apply(manual_tokenize)\n",
    "\n",
    "# Displaying the first few rows with manual tokens\n",
    "data[['cleaned_text', 'manual_tokens']].head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "781b3002-ff6c-4af0-8167-fd23f2ee54cd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>manual_tokens</th>\n",
       "      <th>tokens_no_stopwords_manual</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[i, hope, eveyone, could, get, this, fabulous,...</td>\n",
       "      <td>[hope, eveyone, could, get, fabulous, profit, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[free, signal, future, signal, buy, tia, usdt,...</td>\n",
       "      <td>[free, signal, future, signal, buy, tia, usdt,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[another, 40, profit, after, the, entry, for, ...</td>\n",
       "      <td>[another, 40, profit, entry, buy, signal, cryp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[future, signal, buy, sol, usdt, exchange, all...</td>\n",
       "      <td>[future, signal, buy, sol, usdt, exchange, ava...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[this, is, the, correct, way, of, starting, th...</td>\n",
       "      <td>[correct, way, starting, weekend, ntouched, ta...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       manual_tokens  \\\n",
       "0  [i, hope, eveyone, could, get, this, fabulous,...   \n",
       "1  [free, signal, future, signal, buy, tia, usdt,...   \n",
       "2  [another, 40, profit, after, the, entry, for, ...   \n",
       "3  [future, signal, buy, sol, usdt, exchange, all...   \n",
       "4  [this, is, the, correct, way, of, starting, th...   \n",
       "\n",
       "                          tokens_no_stopwords_manual  \n",
       "0  [hope, eveyone, could, get, fabulous, profit, ...  \n",
       "1  [free, signal, future, signal, buy, tia, usdt,...  \n",
       "2  [another, 40, profit, entry, buy, signal, cryp...  \n",
       "3  [future, signal, buy, sol, usdt, exchange, ava...  \n",
       "4  [correct, way, starting, weekend, ntouched, ta...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Manually defining a basic list of English stopwords\n",
    "basic_stopwords = set([\n",
    "    \"i\", \"me\", \"my\", \"myself\", \"we\", \"our\", \"ours\", \"ourselves\", \"you\", \"your\", \n",
    "    \"yours\", \"yourself\", \"yourselves\", \"he\", \"him\", \"his\", \"himself\", \"she\", \n",
    "    \"her\", \"hers\", \"herself\", \"it\", \"its\", \"itself\", \"they\", \"them\", \"their\", \n",
    "    \"theirs\", \"themselves\", \"what\", \"which\", \"who\", \"whom\", \"this\", \"that\", \n",
    "    \"these\", \"those\", \"am\", \"is\", \"are\", \"was\", \"were\", \"be\", \"been\", \"being\", \n",
    "    \"have\", \"has\", \"had\", \"having\", \"do\", \"does\", \"did\", \"doing\", \"a\", \"an\", \n",
    "    \"the\", \"and\", \"but\", \"if\", \"or\", \"because\", \"as\", \"until\", \"while\", \"of\", \n",
    "    \"at\", \"by\", \"for\", \"with\", \"about\", \"against\", \"between\", \"into\", \"through\", \n",
    "    \"during\", \"before\", \"after\", \"above\", \"below\", \"to\", \"from\", \"up\", \"down\", \n",
    "    \"in\", \"out\", \"on\", \"off\", \"over\", \"under\", \"again\", \"further\", \"then\", \n",
    "    \"once\", \"here\", \"there\", \"when\", \"where\", \"why\", \"how\", \"all\", \"any\", \n",
    "    \"both\", \"each\", \"few\", \"more\", \"most\", \"other\", \"some\", \"such\", \"no\", \n",
    "    \"nor\", \"not\", \"only\", \"own\", \"same\", \"so\", \"than\", \"too\", \"very\", \"s\", \"t\", \n",
    "    \"can\", \"will\", \"just\", \"don\", \"should\", \"now\"\n",
    "])\n",
    "\n",
    "# Function to remove stop words from tokens using the manually defined list\n",
    "def remove_manual_stop_words(tokens):\n",
    "    return [word for word in tokens if word not in basic_stopwords]\n",
    "\n",
    "# Applying the stop word removal function using the manual list\n",
    "data['tokens_no_stopwords_manual'] = data['manual_tokens'].apply(remove_manual_stop_words)\n",
    "\n",
    "# Displaying the first few rows with tokens without stop words\n",
    "data[['manual_tokens', 'tokens_no_stopwords_manual']].head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0b968c8d-b643-4180-a0ba-5f3e0aff49ca",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tokens_no_stopwords_manual</th>\n",
       "      <th>simplified_lemmatized_tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[hope, eveyone, could, get, fabulous, profit, ...</td>\n",
       "      <td>[hope, eveyone, could, get, fabulou, profit, f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[free, signal, future, signal, buy, tia, usdt,...</td>\n",
       "      <td>[free, signal, future, signal, buy, tia, usdt,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[another, 40, profit, entry, buy, signal, cryp...</td>\n",
       "      <td>[another, 40, profit, entry, buy, signal, cryp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[future, signal, buy, sol, usdt, exchange, ava...</td>\n",
       "      <td>[future, signal, buy, sol, usdt, exchange, ava...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[correct, way, starting, weekend, ntouched, ta...</td>\n",
       "      <td>[correct, way, start, weekend, ntouch, target,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          tokens_no_stopwords_manual  \\\n",
       "0  [hope, eveyone, could, get, fabulous, profit, ...   \n",
       "1  [free, signal, future, signal, buy, tia, usdt,...   \n",
       "2  [another, 40, profit, entry, buy, signal, cryp...   \n",
       "3  [future, signal, buy, sol, usdt, exchange, ava...   \n",
       "4  [correct, way, starting, weekend, ntouched, ta...   \n",
       "\n",
       "                        simplified_lemmatized_tokens  \n",
       "0  [hope, eveyone, could, get, fabulou, profit, f...  \n",
       "1  [free, signal, future, signal, buy, tia, usdt,...  \n",
       "2  [another, 40, profit, entry, buy, signal, cryp...  \n",
       "3  [future, signal, buy, sol, usdt, exchange, ava...  \n",
       "4  [correct, way, start, weekend, ntouch, target,...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Simplified lemmatization function (without using WordNet)\n",
    "# This function will perform a basic form of lemmatization based on simple heuristics\n",
    "\n",
    "def simplified_lemmatize(word):\n",
    "    # Basic rules for lemmatization (this is a very rudimentary approach)\n",
    "    if word.endswith('ing'):\n",
    "        return word[:-3]\n",
    "    if word.endswith('ed'):\n",
    "        return word[:-2]\n",
    "    if word.endswith('s'):\n",
    "        return word[:-1]\n",
    "    return word\n",
    "\n",
    "def apply_simplified_lemmatization(tokens):\n",
    "    return [simplified_lemmatize(word) for word in tokens]\n",
    "\n",
    "# Applying simplified lemmatization to the tokens\n",
    "data['simplified_lemmatized_tokens'] = data['tokens_no_stopwords_manual'].apply(apply_simplified_lemmatization)\n",
    "\n",
    "# Displaying the first few rows with simplified lemmatized tokens\n",
    "data[['tokens_no_stopwords_manual', 'simplified_lemmatized_tokens']].head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "481d340c-3513-43a6-9b5a-b6dacbd34252",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(   00  000  01  06  0x  10  100  1000  11  12  ...  wrote  x50  year  years  \\\n",
       " 0   0    0   0   0   0   0    0     0   0   0  ...      0    0     0      0   \n",
       " 1   0    0   0   1   0   6    0     0   1   1  ...      0    0     0      0   \n",
       " 2   0    0   0   0   0   0    0     0   0   0  ...      0    0     0      0   \n",
       " 3   0    0   0   0   0   1    0     0   1   0  ...      0    0     0      0   \n",
       " 4   0    0   0   0   0   0    0     0   0   0  ...      0    0     0      0   \n",
       " \n",
       "    yet  you  your  zero  zhao  zone  \n",
       " 0    0    0     0     0     0     0  \n",
       " 1    0    0     0     0     0     0  \n",
       " 2    0    0     0     0     0     0  \n",
       " 3    0    0     0     0     0     0  \n",
       " 4    0    0     0     0     0     0  \n",
       " \n",
       " [5 rows x 1000 columns],\n",
       "     00  000   01        06   0x        10  100  1000        11        12  ...  \\\n",
       " 0  0.0  0.0  0.0  0.000000  0.0  0.000000  0.0   0.0  0.000000  0.000000  ...   \n",
       " 1  0.0  0.0  0.0  0.147065  0.0  0.475123  0.0   0.0  0.117719  0.104366  ...   \n",
       " 2  0.0  0.0  0.0  0.000000  0.0  0.000000  0.0   0.0  0.000000  0.000000  ...   \n",
       " 3  0.0  0.0  0.0  0.000000  0.0  0.085695  0.0   0.0  0.127394  0.000000  ...   \n",
       " 4  0.0  0.0  0.0  0.000000  0.0  0.000000  0.0   0.0  0.000000  0.000000  ...   \n",
       " \n",
       "    wrote  x50  year  years  yet  you  your  zero  zhao  zone  \n",
       " 0    0.0  0.0   0.0    0.0  0.0  0.0   0.0   0.0   0.0   0.0  \n",
       " 1    0.0  0.0   0.0    0.0  0.0  0.0   0.0   0.0   0.0   0.0  \n",
       " 2    0.0  0.0   0.0    0.0  0.0  0.0   0.0   0.0   0.0   0.0  \n",
       " 3    0.0  0.0   0.0    0.0  0.0  0.0   0.0   0.0   0.0   0.0  \n",
       " 4    0.0  0.0   0.0    0.0  0.0  0.0   0.0   0.0   0.0   0.0  \n",
       " \n",
       " [5 rows x 1000 columns])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "# Creating a CountVectorizer object for BoW\n",
    "bow_vectorizer = CountVectorizer(max_features=1000)  # Limiting to top 1000 features for simplicity\n",
    "bow_features = bow_vectorizer.fit_transform(data['cleaned_text']).toarray()\n",
    "\n",
    "# Creating a TfidfVectorizer object for TF-IDF\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=1000)  # Limiting to top 1000 features\n",
    "tfidf_features = tfidf_vectorizer.fit_transform(data['cleaned_text']).toarray()\n",
    "\n",
    "# Converting the features into DataFrames for better readability\n",
    "bow_df = pd.DataFrame(bow_features, columns=bow_vectorizer.get_feature_names_out())\n",
    "tfidf_df = pd.DataFrame(tfidf_features, columns=tfidf_vectorizer.get_feature_names_out())\n",
    "\n",
    "# Displaying the first few rows of each DataFrame\n",
    "bow_df.head(), tfidf_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b62080d4-863a-47d1-993a-7cdcefcf9723",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cleaned_text</th>\n",
       "      <th>sentiment_score</th>\n",
       "      <th>sentiment_category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>i hope eveyone could get this fabulous profit ...</td>\n",
       "      <td>0.9468</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>free signal future signal buy tia usdt exchan...</td>\n",
       "      <td>0.4767</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>another 40 profit after the entry for a buy si...</td>\n",
       "      <td>0.7351</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>future signal buy sol usdt exchange all avail...</td>\n",
       "      <td>-0.5423</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>this is the correct way of starting the weeken...</td>\n",
       "      <td>0.7351</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>future signal buy sxp usdt exchange all avail...</td>\n",
       "      <td>-0.5423</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>semms ygg doen t have enough strength we just ...</td>\n",
       "      <td>0.9118</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>the price had a bearish shadow don t worry nti...</td>\n",
       "      <td>-0.4404</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>free signal future signal buy ygg usdt exchan...</td>\n",
       "      <td>-0.0516</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>we keep the monday blues away with profitable ...</td>\n",
       "      <td>0.7351</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        cleaned_text  sentiment_score  \\\n",
       "0  i hope eveyone could get this fabulous profit ...           0.9468   \n",
       "1   free signal future signal buy tia usdt exchan...           0.4767   \n",
       "2  another 40 profit after the entry for a buy si...           0.7351   \n",
       "3   future signal buy sol usdt exchange all avail...          -0.5423   \n",
       "4  this is the correct way of starting the weeken...           0.7351   \n",
       "5   future signal buy sxp usdt exchange all avail...          -0.5423   \n",
       "6  semms ygg doen t have enough strength we just ...           0.9118   \n",
       "7  the price had a bearish shadow don t worry nti...          -0.4404   \n",
       "8   free signal future signal buy ygg usdt exchan...          -0.0516   \n",
       "9  we keep the monday blues away with profitable ...           0.7351   \n",
       "\n",
       "  sentiment_category  \n",
       "0           positive  \n",
       "1           positive  \n",
       "2           positive  \n",
       "3           negative  \n",
       "4           positive  \n",
       "5           negative  \n",
       "6           positive  \n",
       "7           negative  \n",
       "8           negative  \n",
       "9           positive  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Initializing VADER's SentimentIntensityAnalyzer\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "\n",
    "# Function to get the compound sentiment score\n",
    "def get_sentiment(text):\n",
    "    return sia.polarity_scores(text)['compound']\n",
    "\n",
    "# Applying the sentiment analysis to the cleaned text\n",
    "data['sentiment_score'] = data['cleaned_text'].apply(get_sentiment)\n",
    "\n",
    "# Categorizing sentiment into positive, neutral, or negative based on the compound score\n",
    "data['sentiment_category'] = data['sentiment_score'].apply(lambda score: 'positive' if score > 0.05 else ('negative' if score < -0.05 else 'neutral'))\n",
    "\n",
    "# Displaying the first few rows with sentiment scores and categories\n",
    "data[['cleaned_text', 'sentiment_score', 'sentiment_category']].head(10)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
